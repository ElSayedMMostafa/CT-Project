{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28363028",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f17218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "from scipy import ndimage\n",
    "import numpy as np\n",
    "\n",
    "# Basics\n",
    "import argparse\n",
    "import os\n",
    "import glob \n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy \n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Functional\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms, models\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# Utilities\n",
    "from PIL import Image \n",
    "from scipy import ndimage\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Transfer Learning\n",
    "from models import resnet\n",
    "\n",
    "# SEED for reproducibility\n",
    "SEED = 25\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "# Setting the DEVICE & BATCH SIZE\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "print(device)\n",
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54355113",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7260aa52",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee781668",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################3#\n",
    "def read_nifti_file(filepath):\n",
    "    \"\"\"Read and load volume\"\"\"\n",
    "    return nib.load(filepath).get_fdata()\n",
    "\n",
    "\n",
    "def normalize(volume):\n",
    "    \"\"\"Normalize the volume\"\"\"\n",
    "    min, max = -1000, 400\n",
    "    volume[volume < min] = min\n",
    "    volume[volume > max] = max\n",
    "    volume = (volume - min) / (max - min)\n",
    "    return volume.astype(\"float32\")\n",
    "\n",
    "\n",
    "def resize_volume(img, dDepth=64, dWidth=128, dHeight=128):\n",
    "    \"\"\"Resize across z-axis\"\"\"\n",
    "    # Get current depth\n",
    "    current_depth = img.shape[-1]\n",
    "    current_width = img.shape[0]\n",
    "    current_height = img.shape[1]\n",
    "    # Compute depth factor\n",
    "    depth = current_depth / dDepth\n",
    "    width = current_width / dWidth\n",
    "    height = current_height / dHeight\n",
    "    depth_factor = 1 / depth\n",
    "    width_factor = 1 / width\n",
    "    height_factor = 1 / height\n",
    "    # Rotate\n",
    "    img = ndimage.rotate(img, 90, reshape=False)\n",
    "    # Resize across z-axis\n",
    "    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)\n",
    "    return img\n",
    "\n",
    "\n",
    "def process_scan(path):\n",
    "    \"\"\"Read and resize volume\"\"\"\n",
    "    # Read scan\n",
    "    volume = read_nifti_file(path)\n",
    "    # Normalize\n",
    "    volume = normalize(volume)\n",
    "    # Resize width, height and depth\n",
    "    volume = resize_volume(volume)\n",
    "    return volume\n",
    "#############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9690b1ef",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dd75d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder \"CT-0\" consist of CT scans having normal lung tissue, no CT-signs of viral pneumonia.\n",
    "normal_scan_paths = [\n",
    "    os.path.join(os.getcwd(), \"MosMedData/CT-0\", x) for x in os.listdir(\"MosMedData/CT-0\")]\n",
    "\n",
    "# Folder \"CT-23\" consist of CT scans having several ground-glass opacifications, involvement of lung parenchyma.\n",
    "abnormal_scan_paths = [\n",
    "    os.path.join(os.getcwd(), \"MosMedData/CT-23\", x) for x in os.listdir(\"MosMedData/CT-23\")]\n",
    "\n",
    "print(\"CT scans with normal lung tissue: \" + str(len(normal_scan_paths)))\n",
    "print(\"CT scans with abnormal lung tissue: \" + str(len(abnormal_scan_paths)))\n",
    "\n",
    "# 1 for abnormal cases & 0 for the normal ones\n",
    "abnormal_labels = [1 for _ in range(len(abnormal_scan_paths))]\n",
    "normal_labels = [0 for _ in range(len(normal_scan_paths))]\n",
    "# Concatenation\n",
    "allPaths = normal_scan_paths + abnormal_scan_paths\n",
    "allLabels = normal_labels + abnormal_labels\n",
    "allPaths = np.array(allPaths)\n",
    "allLabels = np.array(allLabels, dtype=np.int64)\n",
    "print(len(allPaths), len(allLabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3028324",
   "metadata": {},
   "source": [
    "## Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff212774",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    \"\"\"\n",
    "        Customized transform for adding noise\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8bdc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTDataset(Dataset):\n",
    "    \"\"\"\n",
    "        Class for intializing and loading the pytorch dataset --> converted then to dataloader\n",
    "    \"\"\"\n",
    "    def __init__(self, paths, labels, transform=None):\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __nii2tensorarray__(self, data):\n",
    "        [w, h, d] = data.shape\n",
    "        new_data = np.reshape(data, [1, w, h, d])\n",
    "        return new_data\n",
    "    \n",
    "    def __rotate3D(self, volume, angle=65):\n",
    "        volume = np.squeeze(volume)\n",
    "        rotatedVol = ndimage.rotate(volume, angle, reshape=False)\n",
    "        return torch.tensor(np.expand_dims(rotatedVol, axis=0))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.paths.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        scan = process_scan(path)\n",
    "        scan = self.__nii2tensorarray__(scan)    \n",
    "        scan = self.__rotate3D(scan, random.randint(-25, 25)) # added Random ROTATION       \n",
    "        if self.transform:\n",
    "            scan = self.transform(scan)\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        return scan, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9ca8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset object\n",
    "tempDataset = CTDataset(allPaths, allLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7af9c30",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5c1b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "temploader = DataLoader(tempDataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "# Example batch\n",
    "images, _ = next(iter(temploader))\n",
    "# Example 3D volume\n",
    "image = images[0]\n",
    "fig = plt.figure(figsize=(10, 10))  \n",
    "for s in range(image.shape[3]):\n",
    "    plt.subplot(8, 8, s+1)\n",
    "    plt.imshow(np.squeeze(image[:,:,:,s]), cmap=\"gray\")\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca888d37",
   "metadata": {},
   "source": [
    "## Separation of a test set & KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669d949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set!\n",
    "splitting_dev_test = StratifiedShuffleSplit(n_splits=1, test_size=0.15, random_state=SEED)\n",
    "dev_idx, test_idx = next(splitting_dev_test.split(allPaths, allLabels))\n",
    "\n",
    "devPaths, devLabels = allPaths[dev_idx], allLabels[dev_idx]\n",
    "devSet = CTDataset(devPaths, devLabels)\n",
    "devLoader = DataLoader(devSet, batch_size=5, shuffle=True, num_workers=0)\n",
    "\n",
    "testPaths, testLabels = allPaths[test_idx], allLabels[test_idx]\n",
    "testSet = CTDataset(testPaths, testLabels)\n",
    "testLoader = DataLoader(testSet, batch_size=5, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63109eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ec449",
   "metadata": {},
   "source": [
    "# Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b4fe1f",
   "metadata": {},
   "source": [
    "**In this experiment, we try training model from scratch. We use KFold Cross Validation due to the small size of the dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e49311",
   "metadata": {},
   "source": [
    "\n",
    "        * Experiment #0\n",
    "        - In this experiment, we try training model from scratch. \n",
    "        - We use KFold Cross Validation due to the small size of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4450106",
   "metadata": {},
   "source": [
    "    * Experiment #1\n",
    "    - In this experiment, we try training model using pretrained models. \n",
    "    - We use KFold Cross Validation due to the small size of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eab2336",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c1dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTModel(nn.Module):\n",
    "    def __init__(self, input_shape, transferLearning=False):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        # Import the same architecture from MedicalNet\n",
    "        self.tf_model = resnet.resnet10(\n",
    "                sample_input_W=self.input_shape[0],\n",
    "                sample_input_H=self.input_shape[1],\n",
    "                sample_input_D=self.input_shape[2],\n",
    "                shortcut_type='A',\n",
    "                num_seg_classes=2)\n",
    "        if transferLearning:\n",
    "            self.update_transfer_weights()\n",
    "        \"\"\"\n",
    "        # Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
    "        dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None))\n",
    "        \"\"\"\n",
    "        # Adding extra classifier layers\n",
    "        self.classifier= nn.Sequential(\n",
    "            nn.Conv3d(2, 4, 3, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(4, 4, 3, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(4, 4, 3, 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(4, 8, 3, 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.fc1 = nn.Linear(8*3*3, 20)\n",
    "        self.fc2 = nn.Linear(20,5)\n",
    "        self.fc3 = nn.Linear(5, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.tf_model(x)\n",
    "        x = self.classifier(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = Functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = Functional.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = Functional.softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "    def update_transfer_weights(self, weights_path=None):\n",
    "        \"\"\"\n",
    "            This function updates the model weights by pretrained saved weights from MedicalNet.\n",
    "            LINK: https://github.com/Tencent/MedicalNet/tree/18c8bb6cd564eb1b964bffef1f4c2283f1ae6e7b\n",
    "        \"\"\"\n",
    "        self.tf_layers = self.tf_model.state_dict()\n",
    "        if not weights_path:\n",
    "            weights_path = \"./models/resnet_10_23dataset.pth\"\n",
    "        pretrain_weights = torch.load(weights_path)\n",
    "        pretrain_weights = {k[7:]:v for k, v in pretrain_weights['state_dict'].items() if k[:7]==\"module.\"}\n",
    "        pretrain_weights = {k: v for k, v in pretrain_weights.items() if k in self.tf_layers.keys()}\n",
    "        self.tf_layers.update(pretrain_weights)\n",
    "        self.tf_model.load_state_dict(self.tf_layers)\n",
    "        \n",
    "model_tf = CTModel((128,128,64), transferLearning=True)\n",
    "model_tf.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c3d0b4",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ac4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_procedure_withKFolds(net, paths, labels, kfold, stopPatience=3, n_epochs=20, verbose=True, bestModelPath=\"best0\",transfer=\"no\"):\n",
    "    \"\"\"\n",
    "        This function implements the training procedure with KFold cross validation.\n",
    "    \"\"\"\n",
    "    best_loss = 1e+10 # temporary\n",
    "    last_valLoss = 1e+10 # temporary\n",
    "    stoppingCounter = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(net.parameters(), lr=0.0001)\n",
    "    history = {x: {'train': {'loss': [], 'accuracy':[]},\n",
    "                   'val': {'loss': [], 'accuracy':[]}} for x in range(kfold.get_n_splits())}\n",
    "    for nfold, (train_idx, valid_idx) in enumerate(kfold.split(paths, labels)):\n",
    "        trainPaths, trainLabels = paths[train_idx], labels[train_idx]\n",
    "        validPaths, validLabels = paths[valid_idx], labels[valid_idx]\n",
    "\n",
    "        trainSet = CTDataset(trainPaths, trainLabels)\n",
    "        validSet = CTDataset(validPaths, validLabels)\n",
    "\n",
    "        trainLoader = DataLoader(trainSet, batch_size = BATCH_SIZE,shuffle=True)\n",
    "        validLoader = DataLoader(validSet, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        # Start Training!\n",
    "        for epoch in range(n_epochs):\n",
    "            # Define loss & acc variables\n",
    "            epoch_train_loss, epoch_train_acc = 0, 0\n",
    "            epoch_valid_loss, epoch_valid_acc = 0, 0\n",
    "            # Set training mode\n",
    "            net.train()\n",
    "            for inputsTrain, labelsTrain in trainLoader:\n",
    "                inputsTrain, labelsTrain = inputsTrain.to(device), labelsTrain.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputsTrain)\n",
    "                loss = criterion(outputs, labelsTrain)\n",
    "                loss.backward()\n",
    "                optimizer.step() # Update\n",
    "                epoch_train_loss += loss.item() # Accumelate loss values per epoch\n",
    "                # Calculate accurcay\n",
    "                _, indicies = torch.max(outputs, 1)\n",
    "                epoch_train_acc += accuracy_score(labelsTrain.cpu().numpy(), indicies.cpu().numpy())\n",
    "\n",
    "            epoch_train_acc = epoch_train_acc/len(trainLoader)    \n",
    "            epoch_train_loss = epoch_train_loss/len(trainLoader)    \n",
    "            history[nfold]['train']['accuracy'].append(epoch_train_acc)\n",
    "            history[nfold]['train']['loss'].append(epoch_train_loss)\n",
    "\n",
    "            a = np.asarray(history[nfold]['train']['accuracy'])\n",
    "            name3= (\"Fold_{}_train accuracy_{}\".format(nfold,transfer))\n",
    "            with open(name3+\".csv\",\"w\") as fe:\n",
    "                    np.savetxt(fe,a, delimiter=\",\")\n",
    "                \n",
    "            name4= (\"Fold_{}_train loss_{}\".format(nfold,transfer))\n",
    "            a2 = np.asarray(history[nfold]['train']['loss'])\n",
    "            with open(name4+\".csv\",\"w\") as fe:\n",
    "                    np.savetxt(fe, a2, delimiter=\",\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for inputsVal, labelsVal in validLoader: \n",
    "                    inputsVal, labelsVal = inputsVal.to(device), labelsVal.to(device)\n",
    "                    outputsVal = net(inputsVal)\n",
    "                    lossVal = criterion(outputsVal, labelsVal)\n",
    "                    epoch_valid_loss += lossVal.item()\n",
    "                    _, indiciesVal = torch.max(outputsVal, 1)\n",
    "                    epoch_valid_acc += accuracy_score(labelsVal.cpu().numpy(), indiciesVal.cpu().numpy())\n",
    "                    \n",
    "                epoch_valLoss = epoch_valid_loss/len(validLoader)\n",
    "                epoch_valid_acc = epoch_valid_acc/len(validLoader)\n",
    "                history[nfold]['val']['accuracy'].append(epoch_valid_acc)\n",
    "                history[nfold]['val']['loss'].append(epoch_valLoss)\n",
    "                \n",
    "                e = np.asarray(history[nfold]['val']['accuracy'])\n",
    "                name= (\"Fold_{}_Val accuracy_{}\".format(nfold,transfer))\n",
    "                with open(name+\".csv\",\"w\") as fe:\n",
    "                    np.savetxt(fe, e, delimiter=\",\")\n",
    "                \n",
    "                name2= (\"Fold_{}_Val loss_{}\".format(nfold,transfer))\n",
    "                e2 = np.asarray(history[nfold]['val']['loss'])\n",
    "                with open(name2+\".csv\",\"w\") as fe:\n",
    "                    np.savetxt(fe, e2, delimiter=\",\")\n",
    "\n",
    "\n",
    "            if verbose:\n",
    "                print(\"epoch: {}, train_loss: {:0.3f}, train_acc: {:0.3f}, valid_loss: {:0.3f}, valid_acc: {:0.3f}\".format(epoch+1, epoch_train_loss, epoch_train_acc, epoch_valid_loss, epoch_valid_acc))\n",
    "\n",
    "            if epoch_valid_loss <= best_loss:\n",
    "                # Save the model with the lowest validation loss.\n",
    "                torch.save(net.state_dict(), \"./{}.pth\".format(bestModelPath))\n",
    "                best_loss = epoch_valid_loss\n",
    "                if verbose:\n",
    "                    print(\"model is saved...\")\n",
    "            if epoch_valLoss <= last_valLoss:\n",
    "                stoppingCounter = 0\n",
    "            else:\n",
    "                stoppingCounter += 1\n",
    "            last_valLoss = epoch_valLoss\n",
    "\n",
    "            if stoppingCounter >= stopPatience:\n",
    "                if verbose:\n",
    "                    print(\"Early stopped at Fold {}, Epoch {}\".format(nfold, epoch))\n",
    "                stoppingCounter = 0\n",
    "\n",
    "                break\n",
    "    \n",
    "            \n",
    "    return net, history, trainLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3422e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModelPath = \"./LastResults/Transfer/bestModel\"\n",
    "trainedModel_tf, history_tf, tLoader = train_procedure_withKFolds(model_tf, devPaths, devLabels, kfold, stopPatience=5, n_epochs=20, verbose=True, bestModelPath=bestModelPath, transfer=\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff64019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedModel_tf.load_state_dict(torch.load(\"{}.pth\".format(bestModelPath)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070fe10a",
   "metadata": {},
   "source": [
    "## Evalutaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831ddfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotKFoldsHistory(history, type=\"Scratch\", name=\"img\", plot=False):\n",
    "    # First, get the xAxis\n",
    "    xLen = max([len(history[i]['train']['loss']) for i in history.keys()])\n",
    "    xLen = [x+1 for x in range(xLen)]\n",
    "    legends = []\n",
    "\n",
    "    # Validation Loss\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    for iFold in history.keys():\n",
    "        y = history[iFold]['val']['loss']\n",
    "        if len(xLen) != len(y):\n",
    "            newY = [y[i] if i < len(y) else y[-1] for i in range(len(xLen))]\n",
    "            y = newY\n",
    "        plt.plot(xLen, y)\n",
    "        legends.append(\"{}\".format(iFold+1))\n",
    "    plt.title(\"Model Validation Loss\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend(legends)\n",
    "    #plt.savefig('/home/omar.mashaal1/mining/Images/{}/{}.jpg'.format(type, name+\"_valLoss\"))\n",
    "    if plot:\n",
    "        plt.show()\n",
    "\n",
    "    # Validation Accuracy\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    for iFold in history.keys():\n",
    "        y = history[iFold]['val']['accuracy']\n",
    "        if len(xLen) != len(y):\n",
    "            newY = [y[i] if i < len(y) else y[-1] for i in range(len(xLen))]\n",
    "            y = newY\n",
    "        plt.plot(xLen, y)\n",
    "        legends.append(\"{}\".format(iFold+1))\n",
    "    plt.title(\"Model Validation Accuracy\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend(legends)\n",
    "    #plt.savefig('/home/omar.mashaal1/mining/Images/{}/{}.jpg'.format(type, name+\"_valAccuracy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc887443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "def getConfusionMatrix(dataloader, model, type, name, plot=False):\n",
    "    model.eval()\n",
    "    preds, truths = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            print(\"inputs: \", inputs.shape)\n",
    "            _, output = torch.max(model(inputs), 1)\n",
    "            print(\"output: \", output.shape)\n",
    "            preds.extend(output.detach().cpu().numpy())\n",
    "            truths.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(truths, preds)\n",
    "    print(accuracy)\n",
    "    cfMatrix = confusion_matrix(truths, preds)\n",
    "    cfFrame = pd.DataFrame(cfMatrix / np.sum(cfMatrix, axis=1)[:, None], index = [i for i in [0, 1]],\n",
    "                     columns = [i for i in [0, 1]])\n",
    "    plt.figure(figsize = (12,7))\n",
    "    sn.heatmap(cfFrame, annot=True)\n",
    "#     #plt.savefig('/home/omar.mashaal1/mining/Images/{}/{}.jpg'.format(type, name))\n",
    "#     if plot:\n",
    "#         plt.show()\n",
    "    return cfMatrix, truths, preds\n",
    "    #return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054ed6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotKFoldsHistory(history_tf, type=\"TF\", name=\"exp1\", plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0312d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfMatrix_test1, truths_test1, preds_test1 = getConfusionMatrix(testLoader, trainedModel_tf, type=\"TL\", name=\"exp1_testing\", plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31df3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfMatrix_train1, truths_train1, preds_train1 = getConfusionMatrix(devLoader, trainedModel_tf, type=\"TL\", name=\"exp1_training\", plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf1ca19",
   "metadata": {},
   "source": [
    "# Experiment 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd193cc",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85583f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=2)\n",
    "        self.bn = nn.BatchNorm3d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class get_model(nn.Module):\n",
    "    def __init__(self, width=128, height=128, depth=64):\n",
    "        super(get_model, self).__init__()\n",
    "        self.conv1 = ConvBlock(1, 64)\n",
    "        self.conv2 = ConvBlock(64, 64)\n",
    "        self.conv3 = ConvBlock(64, 128)\n",
    "        self.conv4 = ConvBlock(128, 256)\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc1 = nn.Linear(256, 512)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = Functional.softmax(x, dim=1)      \n",
    "        return x\n",
    "Model_0 = get_model().to(device)\n",
    "#count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6017ece8",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca4e8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModelPath = \"./LastResults/Base/bestModel\"\n",
    "trainedModel_0, history_0, tLoader = train_procedure_withKFolds(Model_0, devPaths, devLabels, kfold, stopPatience=5, n_epochs=20, verbose=True, bestModelPath=bestModelPath, transfer=\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc2e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedModel_0.load_state_dict(torch.load(\"{}.pth\".format(bestModelPath)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f87d4ac",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472f6595",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotKFoldsHistory(history_0, type=\"Base\", name=\"exp0\", plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9799be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_test0, T_test0, L_test0 = getConfusionMatrix(testLoader, trainedModel_0, type=\"Base\", name=\"exp0_final\", plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc5a6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.asarray(T_test)\n",
    "name1 = (\"Scratch Best Model Testing_{}\".format(\"Truth\"))\n",
    "with open(name1+\".csv\", \"w\") as fe1:\n",
    "    np.savetxt(fe1, a1, delimiter=\",\")\n",
    "\n",
    "a2 = np.asarray(L_test)\n",
    "name2 = (\"Scratch Best Model Testing_{}\".format(\"Prediction\"))\n",
    "with open(name2+\".csv\", \"w\") as fe2:\n",
    "    np.savetxt(fe2, a2, delimiter=\",\")\n",
    "    \n",
    "a3 = np.asarray(C_test)\n",
    "name3 = (\"Scratch Best Model Testing_{}\".format(\"ConvM\"))\n",
    "with open(name3+\".csv\", \"w\") as fe3:\n",
    "    np.savetxt(fe3, a3, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31527e66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
